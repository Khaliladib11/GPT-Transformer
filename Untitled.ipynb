{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d2eada9-13e3-4ae4-bf4c-f846ff2f06f1",
   "metadata": {},
   "source": [
    "# Exploration Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d65c2-b069-4bf2-a952-0945bf707f9a",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa0785e-50f5-4024-be4b-428a9af24b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd7456-d4ff-4ccf-81be-fedb1781131b",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f8295a4-455a-497f-84fd-bd36a2612685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    with open(data_path, 'r') as file:\n",
    "        data = file.read()\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b70984-09c0-48f8-9180-364aa554b92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Untitled.ipynb', 'README.md', 'input.txt', '.ipynb_checkpoints', '.git']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef4584c-a9ac-49d4-b9fa-54d8a93e82e6",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47358a50-d87e-46a6-a4f7-7bf2f1158227",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_data('input.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d280fc-205a-413e-bb38-929d5aaec151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ac739-c4c2-44db-b3b5-d1f85924592b",
   "metadata": {},
   "source": [
    "##Â Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1963da1-c28f-4367-bce3-e2a6393e2357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 65 vocabulary in our dataset\n",
      "All characters: \n",
      "- -!-$-&-'-,---.-3-:-;-?-A-B-C-D-E-F-G-H-I-J-K-L-M-N-O-P-Q-R-S-T-U-V-W-X-Y-Z-a-b-c-d-e-f-g-h-i-j-k-l-m-n-o-p-q-r-s-t-u-v-w-x-y-z\n"
     ]
    }
   ],
   "source": [
    "all_chars = sorted(list(set(text)))\n",
    "vocab_size = len(all_chars)\n",
    "\n",
    "print(f\"We have {vocab_size} vocabulary in our dataset\")\n",
    "print(f\"All characters: { '-'.join(all_chars) }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb955357-b00b-4ddb-badb-3199eb204336",
   "metadata": {},
   "source": [
    "We must now tokenize the text at character level, in order to do that, we must create:\n",
    "- **Encoder**: which converts the character to numerical representation.\n",
    "- **Decoder**: which converts the numerical representation to character again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19877dc6-043b-4cf6-b9ab-efacda83ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two ways mapping\n",
    "c2i = { char:idx for idx, char in enumerate(all_chars) }\n",
    "i2c = { idx:char for idx, char in enumerate(all_chars) }\n",
    "\n",
    "encoder = lambda string: [ c2i[char] for char in string ]\n",
    "decoder = lambda indexes: \"\".join([ i2c[idx] for idx in indexes ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00b9431c-c3c5-44f0-a349-7a116984827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is Khalil!\n",
      "[20, 43, 50, 50, 53, 1, 51, 63, 1, 52, 39, 51, 43, 1, 47, 57, 1, 23, 46, 39, 50, 47, 50, 2]\n",
      "Hello my name is Khalil!\n"
     ]
    }
   ],
   "source": [
    "test_phrase = \"Hello my name is Khalil!\"\n",
    "print(test_phrase)\n",
    "print(encoder(test_phrase))\n",
    "print(decoder(encoder(test_phrase)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57455ef4-6b10-495d-a280-837649e954c0",
   "metadata": {},
   "source": [
    "Now we will tokenize the whole training set. We will start using **torch** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32c538cf-5b86-4768-a1d3-897fd3b14c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encoder(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efc01d25-eecf-4c8f-87c8-9e64d344404e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've got: torch.Size([1115394]) shape and torch.int64 Tensor's type.\n"
     ]
    }
   ],
   "source": [
    "print(f\"We've got: {data.shape} shape and {data.dtype} Tensor's type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b367c-da3b-41df-84ff-37399fba89e0",
   "metadata": {},
   "source": [
    "## Split Data to Train, Eval and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c624098-1c8e-4222-972b-ba63efb7dad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "782cbf17-f314-433e-8861-181df8c25956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, train_percent, eval_percent, test_percent):\n",
    "    \n",
    "    assert train_percent + eval_percent + test_percent == 1.0, f\"The summation of all percentags must be 1.0, we got {train_percent + eval_percent + test_percent}\"\n",
    "    \n",
    "    train_range = [0, int( len(data) * train_percent )]\n",
    "    eval_range = [ train_range[1], int( len(data) * eval_percentage ) + train_range[1] ]\n",
    "    test_range = [eval_range[1], -1]\n",
    "    \n",
    "    return data[:train_range[1]], data[eval_range[0]:eval_range[1]], data[test_range[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "354cd9f3-e00c-469c-aa9b-25ce6ebbf00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 0.8\n",
    "eval_percentage = 0.1\n",
    "test_percentage = 0.1\n",
    "\n",
    "train_set, eval_set, test_set = split_data(data, train_percentage, eval_percentage, test_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece2fbf1-2d73-41f8-89cc-69497f838746",
   "metadata": {},
   "source": [
    "## Split the data into Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a949f9f2-47d8-483b-9c9d-6ea52254c70f",
   "metadata": {},
   "source": [
    "We don't train the GPT-decoder all at once as it will take a lot of time to do that, instead, we split the data into chunks of **context_length** plus one.\n",
    "\n",
    "You might ask youtself why plus one? The reason why is when at each chunk, we try to predict the i+1. For instance, if the **context_length** is equal to 8:\n",
    "\n",
    "- at i=0, GPT predicts i+1=1\n",
    "- at i=1, GPT preducts i+1=2\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "To reach i=**context_length**, GPT predicts i+1=**context_length** + 1\n",
    "\n",
    "One more thing to mention is we do that as well to make the neural network to learn how to predict the next token from index equals **zero** to index equals **context_length**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "203f553d-1c83-4e7d-a1ca-fbf566d8ced6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is [18], GPT will try to predict 47\n",
      "When the input is [18, 47], GPT will try to predict 56\n",
      "When the input is [18, 47, 56], GPT will try to predict 57\n",
      "When the input is [18, 47, 56, 57], GPT will try to predict 58\n",
      "When the input is [18, 47, 56, 57, 58], GPT will try to predict 1\n",
      "When the input is [18, 47, 56, 57, 58, 1], GPT will try to predict 15\n",
      "When the input is [18, 47, 56, 57, 58, 1, 15], GPT will try to predict 47\n",
      "When the input is [18, 47, 56, 57, 58, 1, 15, 47], GPT will try to predict 58\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_LENGTH = 8\n",
    "\n",
    "x = train_set[:CONTEXT_LENGTH]\n",
    "y = train_set[1:CONTEXT_LENGTH+1]\n",
    "\n",
    "for idx in range(CONTEXT_LENGTH):\n",
    "    context = x[:idx+1].tolist()\n",
    "    target = y[idx]\n",
    "    print(f\"When the input is {context}, GPT will try to predict {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b508a78-0480-49cd-84fe-090df203cb20",
   "metadata": {},
   "source": [
    "Moreover, we will add the batch size. We use batches to stack list of chunks on top of each other in order to keep the GPUS busy all the time. They run independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "80b1426e-5850-4ff3-bc8c-369c2be5c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, batch_size=BATCH_SIZE, context_length=CONTEXT_LENGTH):\n",
    "    indexes = torch.randint( len(data) - batch_size, (batch_size,) )  # get BATCH_SIZE random indexes within the dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # loop through each index and get the context data and finally stack them together to get torch.tensor of shape (BATCH_SIZE, CONTEXT_LENGTH) \n",
    "    x = torch.stack( [ data[ idx: idx+context_length ] for idx in indexes ] )\n",
    "    # loop through each index and get the target data and finally stack them together to get torch.tensor of shape (BATCH_SIZE, CONTEXT_LENGTH)\n",
    "    y = torch.stack( [ data[idx+1:idx+context_length+1] for idx in indexes ] )\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4661d241-353a-46f5-bad1-4f0f5186e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  torch.Size([4, 8])\n",
      "y shape:  torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "BATCH_SIZE = 4\n",
    "x, y = get_batch(train_set)\n",
    "print(\"x shape: \", x.shape)\n",
    "print(\"y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7a81d760-aa0a-48b6-bcc4-582617bf4384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[61, 46, 47, 54,  1, 63, 53, 59],\n",
       "        [63,  1, 57, 43, 58,  1, 42, 53],\n",
       "        [58, 46, 47, 52, 49,  1, 63, 53],\n",
       "        [63,  1, 61, 47, 52, 45,  6,  0]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "46c30466-6373-4da3-8e49-aefb6d81e01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[46, 47, 54,  1, 63, 53, 59,  1],\n",
       "        [ 1, 57, 43, 58,  1, 42, 53, 61],\n",
       "        [46, 47, 52, 49,  1, 63, 53, 59],\n",
       "        [ 1, 61, 47, 52, 45,  6,  0, 22]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7a53cd3d-9bc4-4791-b3e0-1164dc644122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([61]), Output: 46\n",
      "Input: tensor([61, 46]), Output: 47\n",
      "Input: tensor([61, 46, 47]), Output: 54\n",
      "Input: tensor([61, 46, 47, 54]), Output: 1\n",
      "Input: tensor([61, 46, 47, 54,  1]), Output: 63\n",
      "Input: tensor([61, 46, 47, 54,  1, 63]), Output: 53\n",
      "Input: tensor([61, 46, 47, 54,  1, 63, 53]), Output: 59\n",
      "Input: tensor([61, 46, 47, 54,  1, 63, 53, 59]), Output: 1\n",
      "\n",
      "Input: tensor([63]), Output: 1\n",
      "Input: tensor([63,  1]), Output: 57\n",
      "Input: tensor([63,  1, 57]), Output: 43\n",
      "Input: tensor([63,  1, 57, 43]), Output: 58\n",
      "Input: tensor([63,  1, 57, 43, 58]), Output: 1\n",
      "Input: tensor([63,  1, 57, 43, 58,  1]), Output: 42\n",
      "Input: tensor([63,  1, 57, 43, 58,  1, 42]), Output: 53\n",
      "Input: tensor([63,  1, 57, 43, 58,  1, 42, 53]), Output: 61\n",
      "\n",
      "Input: tensor([58]), Output: 46\n",
      "Input: tensor([58, 46]), Output: 47\n",
      "Input: tensor([58, 46, 47]), Output: 52\n",
      "Input: tensor([58, 46, 47, 52]), Output: 49\n",
      "Input: tensor([58, 46, 47, 52, 49]), Output: 1\n",
      "Input: tensor([58, 46, 47, 52, 49,  1]), Output: 63\n",
      "Input: tensor([58, 46, 47, 52, 49,  1, 63]), Output: 53\n",
      "Input: tensor([58, 46, 47, 52, 49,  1, 63, 53]), Output: 59\n",
      "\n",
      "Input: tensor([63]), Output: 1\n",
      "Input: tensor([63,  1]), Output: 61\n",
      "Input: tensor([63,  1, 61]), Output: 47\n",
      "Input: tensor([63,  1, 61, 47]), Output: 52\n",
      "Input: tensor([63,  1, 61, 47, 52]), Output: 45\n",
      "Input: tensor([63,  1, 61, 47, 52, 45]), Output: 6\n",
      "Input: tensor([63,  1, 61, 47, 52, 45,  6]), Output: 0\n",
      "Input: tensor([63,  1, 61, 47, 52, 45,  6,  0]), Output: 22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in range(BATCH_SIZE):\n",
    "    for c in range(CONTEXT_LENGTH):\n",
    "        context = x[b, :c+1]\n",
    "        target = y[b, c]\n",
    "        print(f\"Input: {context}, Output: {target}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56709e71-3f1f-4340-8675-54b6ce21c154",
   "metadata": {},
   "source": [
    "## Build Bigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d34ee8-2673-49fb-a7f5-7a1949154f88",
   "metadata": {},
   "source": [
    "Now let's build the **Bigram Language Model** to feed the batches to it.\n",
    "\n",
    "For the lose function, we will use the **Negative log likelihood** function to calculate the loss between logits and target. To do that we can use **F.cross_entropy** from **torch**.\n",
    "\n",
    "**cross_entropy** expects the **vocab_size** to be the second dims, thus we will reshape the inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "23d4761f-1159-4f45-9113-2faff644ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, context, targets=None):\n",
    "        \n",
    "        # this returns a torch.tensor with shape of (BATCH_SIZE, CONTEXT_LENGTH, VOCAB_SIZE)\n",
    "        # e.g. (4, 8, 65)\n",
    "        logits = self.embedding_table(context)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            B, C, V = logits.shape\n",
    "\n",
    "            # we're going to strech out the array, new shape: (BATCH_SIZE * CONTEXT_LENGTH, VOCAB_SIZE)\n",
    "            # e.g. (4*8, 65) == (32, 65)\n",
    "            logits = logits.view(B*C, V)\n",
    "\n",
    "            # and for the targets as well, we're going to change it's shape to be one dim\n",
    "            # e.g. (32)\n",
    "            targets = targets.view(B*C)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def generate(self, context, max_tokens):\n",
    "        # Fist of all the context is with (B, C) shape\n",
    "        for _ in range(max_tokens):\n",
    "            # we get the prediction, the logits will be in (B, C, V) shape and the loss will be None\n",
    "            logits, loss = self(context)\n",
    "            # Focus only on the last character, this will change later\n",
    "            logits = logits[:, -1, :]\n",
    "            # get the probability distribuation where the sum of probabilities are equal to 1\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # get random sample distribution from the probability\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # concatinate the generated token with the previous set of tokens\n",
    "            context = torch.cat((context, next_token), dim=1)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e78f09b0-3c23-411e-aad2-06e4a3043c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8904, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bigram_model = BigramLanguageModel(vocab_size)\n",
    "\n",
    "logits, loss = bigram_model(x, y)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ab0c2503-860e-41bc-8e7e-539955de27c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 48, 23, 3, 7, 13, 7, 61, 2, 28, 1, 21, 52, 38, 27, 51, 59, 44, 14, 26, 39, 2, 53, 33, 13, 35, 39, 2, 61, 39, 2, 33, 41, 52, 2, 0, 42, 45, 55, 33, 62, 22, 11, 21, 4, 59, 31, 55, 17, 4, 53, 32, 10, 64, 37, 15, 22, 48, 20, 26, 62, 63, 48, 18, 39, 7, 60, 31, 28, 21, 49, 55, 54, 44, 1, 47, 26, 37, 8, 21, 18, 3, 45, 20, 5, 62, 55, 58, 30, 58, 43, 46, 3, 22, 11, 39, 36, 50, 41, 62, 64]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\njK$-A-w!P InZOmufBNa!oUAWa!wa!Ucn!\\ndgqUxJ;I&uSqE&oT:zYCJjHNxyjFa-vSPIkqpf iNY.IF$gH'xqtRteh$J;aXlcxz\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "test_idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_results = bigram_model.generate(test_idx, max_tokens=100)[0].tolist()\n",
    "print(generated_results)\n",
    "decoder(generated_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf5aac-59bc-4758-b46a-387285bb7805",
   "metadata": {},
   "source": [
    "## Training the Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "01bf4dc6-c7fc-426e-be9f-67cb7dd26ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an optimizer\n",
    "optimizer = torch.optim.AdamW(bigram_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8afdd5dc-1def-4484-9eb1-52bf695988fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, data, epochs, optimizer, batch_size, context_length):\n",
    "    traning_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        x, y = get_batch(data, batch_size, context_length)\n",
    "        \n",
    "        logits, loss = model(x, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        traning_loss.append(loss.item())\n",
    "        \n",
    "    return traning_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b353d99d-5ce5-47f6-b194-9c836a925ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "traning_loss = training_loop(bigram_model, train_set, 10000, optimizer, 32, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2a11f8dc-93c1-403c-8aed-663b1c398dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.400531530380249"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traning_loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c017fe0c-4fe4-462f-bc86-ec3d70652b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nASe hou l ROLO:\\nThe sthor he, momy RENourdeld\\nWiere RWirear erinst wnge hed ly wnilobea oullof If irs f ankee iceyodatay l wiour be athe myo m.\\n\\nI t,\\nLO D tho tere FFin I the ked thes th ury henitwand f h,\\nINI t,\\nKIsis f d'men y s maiothe Ifors.\\nINVisusou cilonthipous jonousemie t tispo y es hy t so\""
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "test_idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_results = bigram_model.generate(test_idx, max_tokens=300)[0].tolist()\n",
    "decoder(generated_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
