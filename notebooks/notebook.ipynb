{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d2eada9-13e3-4ae4-bf4c-f846ff2f06f1",
   "metadata": {},
   "source": [
    "# Exploration Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d65c2-b069-4bf2-a952-0945bf707f9a",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa0785e-50f5-4024-be4b-428a9af24b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea165c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd7456-d4ff-4ccf-81be-fedb1781131b",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f8295a4-455a-497f-84fd-bd36a2612685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    with open(data_path, 'r') as file:\n",
    "        data = file.read()\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61b70984-09c0-48f8-9180-364aa554b92a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test',\n",
       " 'k.txt',\n",
       " 'notebook.ipynb',\n",
       " 'README.md',\n",
       " 'input.txt',\n",
       " '.gitignore',\n",
       " '.ipynb_checkpoints',\n",
       " '.git',\n",
       " 'src']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef4584c-a9ac-49d4-b9fa-54d8a93e82e6",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47358a50-d87e-46a6-a4f7-7bf2f1158227",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_data('input.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8d280fc-205a-413e-bb38-929d5aaec151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ac739-c4c2-44db-b3b5-d1f85924592b",
   "metadata": {},
   "source": [
    "##Â Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1963da1-c28f-4367-bce3-e2a6393e2357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 65 vocabulary in our dataset\n",
      "All characters: \n",
      "- -!-$-&-'-,---.-3-:-;-?-A-B-C-D-E-F-G-H-I-J-K-L-M-N-O-P-Q-R-S-T-U-V-W-X-Y-Z-a-b-c-d-e-f-g-h-i-j-k-l-m-n-o-p-q-r-s-t-u-v-w-x-y-z\n"
     ]
    }
   ],
   "source": [
    "all_chars = sorted(list(set(text)))\n",
    "vocab_size = len(all_chars)\n",
    "\n",
    "print(f\"We have {vocab_size} vocabulary in our dataset\")\n",
    "print(f\"All characters: { '-'.join(all_chars) }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb955357-b00b-4ddb-badb-3199eb204336",
   "metadata": {},
   "source": [
    "We must now tokenize the text at character level, in order to do that, we must create:\n",
    "- **Encoder**: which converts the character to numerical representation.\n",
    "- **Decoder**: which converts the numerical representation to character again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19877dc6-043b-4cf6-b9ab-efacda83ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two ways mapping\n",
    "c2i = { char:idx for idx, char in enumerate(all_chars) }\n",
    "i2c = { idx:char for idx, char in enumerate(all_chars) }\n",
    "\n",
    "encoder = lambda string: [ c2i[char] for char in string ]\n",
    "decoder = lambda indexes: \"\".join([ i2c[idx] for idx in indexes ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00b9431c-c3c5-44f0-a349-7a116984827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is Khalil!\n",
      "[20, 43, 50, 50, 53, 1, 51, 63, 1, 52, 39, 51, 43, 1, 47, 57, 1, 23, 46, 39, 50, 47, 50, 2]\n",
      "Hello my name is Khalil!\n"
     ]
    }
   ],
   "source": [
    "test_phrase = \"Hello my name is Khalil!\"\n",
    "print(test_phrase)\n",
    "print(encoder(test_phrase))\n",
    "print(decoder(encoder(test_phrase)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57455ef4-6b10-495d-a280-837649e954c0",
   "metadata": {},
   "source": [
    "Now we will tokenize the whole training set. We will start using **torch** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32c538cf-5b86-4768-a1d3-897fd3b14c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encoder(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efc01d25-eecf-4c8f-87c8-9e64d344404e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've got: torch.Size([1115394]) shape and torch.int64 Tensor's type.\n"
     ]
    }
   ],
   "source": [
    "print(f\"We've got: {data.shape} shape and {data.dtype} Tensor's type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b367c-da3b-41df-84ff-37399fba89e0",
   "metadata": {},
   "source": [
    "## Split Data to Train, Eval and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c624098-1c8e-4222-972b-ba63efb7dad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "782cbf17-f314-433e-8861-181df8c25956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, train_percent, eval_percent, test_percent):\n",
    "    \n",
    "    assert train_percent + eval_percent + test_percent == 1.0, f\"The summation of all percentags must be 1.0, we got {train_percent + eval_percent + test_percent}\"\n",
    "    \n",
    "    train_range = [0, int( len(data) * train_percent )]\n",
    "    eval_range = [ train_range[1], int( len(data) * eval_percentage ) + train_range[1] ]\n",
    "    test_range = [eval_range[1], -1]\n",
    "    \n",
    "    return data[:train_range[1]], data[eval_range[0]:eval_range[1]], data[test_range[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "354cd9f3-e00c-469c-aa9b-25ce6ebbf00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 0.8\n",
    "eval_percentage = 0.1\n",
    "test_percentage = 0.1\n",
    "\n",
    "train_set, eval_set, test_set = split_data(data, train_percentage, eval_percentage, test_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece2fbf1-2d73-41f8-89cc-69497f838746",
   "metadata": {},
   "source": [
    "## Split the data into Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a949f9f2-47d8-483b-9c9d-6ea52254c70f",
   "metadata": {},
   "source": [
    "We don't train the GPT-decoder all at once as it will take a lot of time to do that, instead, we split the data into chunks of **context_length** plus one.\n",
    "\n",
    "You might ask youtself why plus one? The reason why is when at each chunk, we try to predict the i+1. For instance, if the **context_length** is equal to 8:\n",
    "\n",
    "- at i=0, GPT predicts i+1=1\n",
    "- at i=1, GPT preducts i+1=2\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "To reach i=**context_length**, GPT predicts i+1=**context_length** + 1\n",
    "\n",
    "One more thing to mention is we do that as well to make the neural network to learn how to predict the next token from index equals **zero** to index equals **context_length**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "203f553d-1c83-4e7d-a1ca-fbf566d8ced6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is [18], GPT will try to predict 47\n",
      "When the input is [18, 47], GPT will try to predict 56\n",
      "When the input is [18, 47, 56], GPT will try to predict 57\n",
      "When the input is [18, 47, 56, 57], GPT will try to predict 58\n",
      "When the input is [18, 47, 56, 57, 58], GPT will try to predict 1\n",
      "When the input is [18, 47, 56, 57, 58, 1], GPT will try to predict 15\n",
      "When the input is [18, 47, 56, 57, 58, 1, 15], GPT will try to predict 47\n",
      "When the input is [18, 47, 56, 57, 58, 1, 15, 47], GPT will try to predict 58\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_LENGTH = 8\n",
    "\n",
    "x = train_set[:CONTEXT_LENGTH]\n",
    "y = train_set[1:CONTEXT_LENGTH+1]\n",
    "\n",
    "for idx in range(CONTEXT_LENGTH):\n",
    "    context = x[:idx+1].tolist()\n",
    "    target = y[idx]\n",
    "    print(f\"When the input is {context}, GPT will try to predict {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b508a78-0480-49cd-84fe-090df203cb20",
   "metadata": {},
   "source": [
    "Moreover, we will add the batch size. We use batches to stack list of chunks on top of each other in order to keep the GPUS busy all the time. They run independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80b1426e-5850-4ff3-bc8c-369c2be5c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "def get_batch(data, batch_size=BATCH_SIZE, context_length=CONTEXT_LENGTH):\n",
    "    indexes = torch.randint( len(data) - batch_size, (batch_size,) )  # get BATCH_SIZE random indexes within the dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # loop through each index and get the context data and finally stack them together to get torch.tensor of shape (BATCH_SIZE, CONTEXT_LENGTH) \n",
    "    x = torch.stack( [ data[ idx: idx+context_length ] for idx in indexes ] )\n",
    "    # loop through each index and get the target data and finally stack them together to get torch.tensor of shape (BATCH_SIZE, CONTEXT_LENGTH)\n",
    "    y = torch.stack( [ data[idx+1:idx+context_length+1] for idx in indexes ] )\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4661d241-353a-46f5-bad1-4f0f5186e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  torch.Size([4, 8])\n",
      "y shape:  torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "\n",
    "x, y = get_batch(train_set)\n",
    "print(\"x shape: \", x.shape)\n",
    "print(\"y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a81d760-aa0a-48b6-bcc4-582617bf4384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[61, 46, 47, 54,  1, 63, 53, 59],\n",
       "        [63,  1, 57, 43, 58,  1, 42, 53],\n",
       "        [58, 46, 47, 52, 49,  1, 63, 53],\n",
       "        [63,  1, 61, 47, 52, 45,  6,  0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46c30466-6373-4da3-8e49-aefb6d81e01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[46, 47, 54,  1, 63, 53, 59,  1],\n",
       "        [ 1, 57, 43, 58,  1, 42, 53, 61],\n",
       "        [46, 47, 52, 49,  1, 63, 53, 59],\n",
       "        [ 1, 61, 47, 52, 45,  6,  0, 22]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a53cd3d-9bc4-4791-b3e0-1164dc644122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([61]), Output: 46\n",
      "Input: tensor([61, 46]), Output: 47\n",
      "Input: tensor([61, 46, 47]), Output: 54\n",
      "Input: tensor([61, 46, 47, 54]), Output: 1\n",
      "Input: tensor([61, 46, 47, 54,  1]), Output: 63\n",
      "Input: tensor([61, 46, 47, 54,  1, 63]), Output: 53\n",
      "Input: tensor([61, 46, 47, 54,  1, 63, 53]), Output: 59\n",
      "Input: tensor([61, 46, 47, 54,  1, 63, 53, 59]), Output: 1\n",
      "\n",
      "Input: tensor([63]), Output: 1\n",
      "Input: tensor([63,  1]), Output: 57\n",
      "Input: tensor([63,  1, 57]), Output: 43\n",
      "Input: tensor([63,  1, 57, 43]), Output: 58\n",
      "Input: tensor([63,  1, 57, 43, 58]), Output: 1\n",
      "Input: tensor([63,  1, 57, 43, 58,  1]), Output: 42\n",
      "Input: tensor([63,  1, 57, 43, 58,  1, 42]), Output: 53\n",
      "Input: tensor([63,  1, 57, 43, 58,  1, 42, 53]), Output: 61\n",
      "\n",
      "Input: tensor([58]), Output: 46\n",
      "Input: tensor([58, 46]), Output: 47\n",
      "Input: tensor([58, 46, 47]), Output: 52\n",
      "Input: tensor([58, 46, 47, 52]), Output: 49\n",
      "Input: tensor([58, 46, 47, 52, 49]), Output: 1\n",
      "Input: tensor([58, 46, 47, 52, 49,  1]), Output: 63\n",
      "Input: tensor([58, 46, 47, 52, 49,  1, 63]), Output: 53\n",
      "Input: tensor([58, 46, 47, 52, 49,  1, 63, 53]), Output: 59\n",
      "\n",
      "Input: tensor([63]), Output: 1\n",
      "Input: tensor([63,  1]), Output: 61\n",
      "Input: tensor([63,  1, 61]), Output: 47\n",
      "Input: tensor([63,  1, 61, 47]), Output: 52\n",
      "Input: tensor([63,  1, 61, 47, 52]), Output: 45\n",
      "Input: tensor([63,  1, 61, 47, 52, 45]), Output: 6\n",
      "Input: tensor([63,  1, 61, 47, 52, 45,  6]), Output: 0\n",
      "Input: tensor([63,  1, 61, 47, 52, 45,  6,  0]), Output: 22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in range(BATCH_SIZE):\n",
    "    for c in range(CONTEXT_LENGTH):\n",
    "        context = x[b, :c+1]\n",
    "        target = y[b, c]\n",
    "        print(f\"Input: {context}, Output: {target}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56709e71-3f1f-4340-8675-54b6ce21c154",
   "metadata": {},
   "source": [
    "## Build Bigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d34ee8-2673-49fb-a7f5-7a1949154f88",
   "metadata": {},
   "source": [
    "Now let's build the **Bigram Language Model** to feed the batches to it.\n",
    "\n",
    "For the lose function, we will use the **Negative log likelihood** function to calculate the loss between logits and target. To do that we can use **F.cross_entropy** from **torch**.\n",
    "\n",
    "**cross_entropy** expects the **vocab_size** to be the second dims, thus we will reshape the inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23d4761f-1159-4f45-9113-2faff644ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, context, targets=None):\n",
    "        \n",
    "        # this returns a torch.tensor with shape of (BATCH_SIZE, CONTEXT_LENGTH, VOCAB_SIZE)\n",
    "        # e.g. (4, 8, 65)\n",
    "        logits = self.embedding_table(context)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            B, C, V = logits.shape\n",
    "\n",
    "            # we're going to strech out the array, new shape: (BATCH_SIZE * CONTEXT_LENGTH, VOCAB_SIZE)\n",
    "            # e.g. (4*8, 65) == (32, 65)\n",
    "            logits = logits.view(B*C, V)\n",
    "\n",
    "            # and for the targets as well, we're going to change it's shape to be one dim\n",
    "            # e.g. (32)\n",
    "            targets = targets.view(B*C)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def generate(self, context, max_tokens):\n",
    "        # Fist of all the context is with (B, C) shape\n",
    "        for _ in range(max_tokens):\n",
    "            # we get the prediction, the logits will be in (B, C, V) shape and the loss will be None\n",
    "            logits, loss = self(context)\n",
    "            # Focus only on the last character, this will change later\n",
    "            logits = logits[:, -1, :]\n",
    "            # get the probability distribuation where the sum of probabilities are equal to 1\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # get random sample distribution from the probability\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # concatinate the generated token with the previous set of tokens\n",
    "            context = torch.cat((context, next_token), dim=1)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e78f09b0-3c23-411e-aad2-06e4a3043c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8904, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bigram_model = BigramLanguageModel(vocab_size)\n",
    "\n",
    "logits, loss = bigram_model(x, y)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab0c2503-860e-41bc-8e7e-539955de27c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 31, 56, 12, 55, 28, 7, 29, 35, 49, 58, 36, 53, 24, 4, 48, 24, 16, 22, 45, 27, 24, 34, 64, 5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34, 4, 60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63, 39, 53, 8, 55, 44, 64, 57, 3, 37, 57, 3, 64, 18, 7, 61, 6, 11, 43, 17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57, 2, 47, 35, 35, 8, 27, 40, 64, 16, 52, 62, 13, 1, 25, 57, 3, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nSr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "test_idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_results = bigram_model.generate(test_idx, max_tokens=100)[0].tolist()\n",
    "print(generated_results)\n",
    "decoder(generated_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf5aac-59bc-4758-b46a-387285bb7805",
   "metadata": {},
   "source": [
    "## Training the Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8afdd5dc-1def-4484-9eb1-52bf695988fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, data, epochs, optimizer, batch_size, context_length, device):\n",
    "    training_loss = []\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        x, y = get_batch(data, batch_size, context_length)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss.append(loss.item())\n",
    "\n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60943be7-67ab-4523-9c09-bc865da2eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an optimizer\n",
    "device = 'mps'\n",
    "bigram_model = bigram_model.to(device)\n",
    "optimizer = torch.optim.AdamW(bigram_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b353d99d-5ce5-47f6-b194-9c836a925ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "traning_loss = training_loop(bigram_model, train_set, 100, optimizer, 32, 8, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a11f8dc-93c1-403c-8aed-663b1c398dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.656949520111084"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traning_loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c017fe0c-4fe4-462f-bc86-ec3d70652b88",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[1;32m      2\u001b[0m test_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m----> 3\u001b[0m generated_results \u001b[38;5;241m=\u001b[39m \u001b[43mbigram_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      4\u001b[0m decoder(generated_results)\n",
      "Cell \u001b[0;32mIn[22], line 42\u001b[0m, in \u001b[0;36mBigramLanguageModel.generate\u001b[0;34m(self, context, max_tokens)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, context, max_tokens):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Fist of all the context is with (B, C) shape\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_tokens):\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;66;03m# we get the prediction, the logits will be in (B, C, V) shape and the loss will be None\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;66;03m# Focus only on the last character, this will change later\u001b[39;00m\n\u001b[1;32m     44\u001b[0m         logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch2.0/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[22], line 17\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[0;34m(self, context, targets)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, context, targets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     14\u001b[0m     \n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# this returns a torch.tensor with shape of (BATCH_SIZE, CONTEXT_LENGTH, VOCAB_SIZE)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# e.g. (4, 8, 65)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch2.0/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch2.0/lib/python3.9/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch2.0/lib/python3.9/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_results = bigram_model.generate(test_idx, max_tokens=300)[0].tolist()\n",
    "decoder(generated_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "eedc6156-3014-43d6-8986-ca2e1491ff68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 13,\n",
       " 31,\n",
       " 43,\n",
       " 1,\n",
       " 46,\n",
       " 53,\n",
       " 59,\n",
       " 1,\n",
       " 50,\n",
       " 1,\n",
       " 30,\n",
       " 27,\n",
       " 24,\n",
       " 27,\n",
       " 10,\n",
       " 0,\n",
       " 32,\n",
       " 46,\n",
       " 43,\n",
       " 1,\n",
       " 57,\n",
       " 58,\n",
       " 46,\n",
       " 53,\n",
       " 56,\n",
       " 1,\n",
       " 46,\n",
       " 43,\n",
       " 6,\n",
       " 1,\n",
       " 51,\n",
       " 53,\n",
       " 51,\n",
       " 63,\n",
       " 1,\n",
       " 30,\n",
       " 17,\n",
       " 26,\n",
       " 53,\n",
       " 59,\n",
       " 56,\n",
       " 42,\n",
       " 43,\n",
       " 50,\n",
       " 42,\n",
       " 0,\n",
       " 35,\n",
       " 47,\n",
       " 43,\n",
       " 56,\n",
       " 43,\n",
       " 1,\n",
       " 30,\n",
       " 35,\n",
       " 47,\n",
       " 56,\n",
       " 43,\n",
       " 39,\n",
       " 56,\n",
       " 1,\n",
       " 43,\n",
       " 56,\n",
       " 47,\n",
       " 52,\n",
       " 57,\n",
       " 58,\n",
       " 1,\n",
       " 61,\n",
       " 52,\n",
       " 45,\n",
       " 43,\n",
       " 1,\n",
       " 46,\n",
       " 43,\n",
       " 42,\n",
       " 1,\n",
       " 50,\n",
       " 63,\n",
       " 1,\n",
       " 61,\n",
       " 52,\n",
       " 47,\n",
       " 50,\n",
       " 53,\n",
       " 40,\n",
       " 43,\n",
       " 39,\n",
       " 1,\n",
       " 53,\n",
       " 59,\n",
       " 50,\n",
       " 50,\n",
       " 53,\n",
       " 44,\n",
       " 1,\n",
       " 21,\n",
       " 44,\n",
       " 1,\n",
       " 47,\n",
       " 56,\n",
       " 57,\n",
       " 1,\n",
       " 44,\n",
       " 1,\n",
       " 39,\n",
       " 52,\n",
       " 49,\n",
       " 43,\n",
       " 43,\n",
       " 1,\n",
       " 47,\n",
       " 41,\n",
       " 43,\n",
       " 63,\n",
       " 53,\n",
       " 42,\n",
       " 39,\n",
       " 58,\n",
       " 39,\n",
       " 63,\n",
       " 1,\n",
       " 50,\n",
       " 1,\n",
       " 61,\n",
       " 47,\n",
       " 53,\n",
       " 59,\n",
       " 56,\n",
       " 1,\n",
       " 40,\n",
       " 43,\n",
       " 1,\n",
       " 39,\n",
       " 58,\n",
       " 46,\n",
       " 43,\n",
       " 1,\n",
       " 51,\n",
       " 63,\n",
       " 53,\n",
       " 1,\n",
       " 51,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 21,\n",
       " 1,\n",
       " 58,\n",
       " 6,\n",
       " 0,\n",
       " 24,\n",
       " 27,\n",
       " 1,\n",
       " 16,\n",
       " 1,\n",
       " 58,\n",
       " 46,\n",
       " 53,\n",
       " 1,\n",
       " 58,\n",
       " 43,\n",
       " 56,\n",
       " 43,\n",
       " 1,\n",
       " 18,\n",
       " 18,\n",
       " 47,\n",
       " 52,\n",
       " 1,\n",
       " 21,\n",
       " 1,\n",
       " 58,\n",
       " 46,\n",
       " 43,\n",
       " 1,\n",
       " 49,\n",
       " 43,\n",
       " 42,\n",
       " 1,\n",
       " 58,\n",
       " 46,\n",
       " 43,\n",
       " 57,\n",
       " 1,\n",
       " 58,\n",
       " 46,\n",
       " 1,\n",
       " 59,\n",
       " 56,\n",
       " 63,\n",
       " 1,\n",
       " 46,\n",
       " 43,\n",
       " 52,\n",
       " 47,\n",
       " 58,\n",
       " 61,\n",
       " 39,\n",
       " 52,\n",
       " 42,\n",
       " 1,\n",
       " 44,\n",
       " 1,\n",
       " 46,\n",
       " 6,\n",
       " 0,\n",
       " 21,\n",
       " 26,\n",
       " 21,\n",
       " 1,\n",
       " 58,\n",
       " 6,\n",
       " 0,\n",
       " 23,\n",
       " 21,\n",
       " 57,\n",
       " 47,\n",
       " 57,\n",
       " 1,\n",
       " 44,\n",
       " 1,\n",
       " 42,\n",
       " 5,\n",
       " 51,\n",
       " 43,\n",
       " 52,\n",
       " 1,\n",
       " 63,\n",
       " 1,\n",
       " 57,\n",
       " 1,\n",
       " 51,\n",
       " 39,\n",
       " 47,\n",
       " 53,\n",
       " 58,\n",
       " 46,\n",
       " 43,\n",
       " 1,\n",
       " 21,\n",
       " 44,\n",
       " 53,\n",
       " 56,\n",
       " 57,\n",
       " 8,\n",
       " 0,\n",
       " 21,\n",
       " 26,\n",
       " 34,\n",
       " 47,\n",
       " 57,\n",
       " 59,\n",
       " 57,\n",
       " 53,\n",
       " 59,\n",
       " 1,\n",
       " 41,\n",
       " 47,\n",
       " 50,\n",
       " 53,\n",
       " 52,\n",
       " 58,\n",
       " 46,\n",
       " 47,\n",
       " 54,\n",
       " 53,\n",
       " 59,\n",
       " 57,\n",
       " 1,\n",
       " 48,\n",
       " 53,\n",
       " 52,\n",
       " 53,\n",
       " 59,\n",
       " 57,\n",
       " 43,\n",
       " 51,\n",
       " 47,\n",
       " 43,\n",
       " 1,\n",
       " 58,\n",
       " 1,\n",
       " 58,\n",
       " 47,\n",
       " 57,\n",
       " 54,\n",
       " 53,\n",
       " 1,\n",
       " 63,\n",
       " 1,\n",
       " 43,\n",
       " 57,\n",
       " 1,\n",
       " 46,\n",
       " 63,\n",
       " 1,\n",
       " 58,\n",
       " 1,\n",
       " 57,\n",
       " 53]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c13debff-67a1-4ec3-a779-8dd5e7719145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.has_mps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
